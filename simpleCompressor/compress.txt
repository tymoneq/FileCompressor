`18 computer science `37 `102 theory, a `15 `56 `104 a particular type `118 `120 prefix `56 that `104 commonly used `86 lossless data compression. `28 process `118 `85 `122 `176 such a `56 `104 `15 coding, an `31 developed by `11 `5 `15 while `95 `180 a Sc.D. student at MIT, `37 published `101 `161 1952 `126 "A Method `86 `161 Construction `118 Minimum-Redundancy Codes".[1] `28 output `89 `16 `31 `52 `46 viewed `42 a variable-length `56 `157 `86 `78 a `147 symbol (such `42 a character `101 a file). `28 `31 derives `165 `157 `89 `161 estimated probability `122 frequency `118 occurrence (weight) `86 each possible value `118 `161 `147 symbol. As `101 other entropy `78 methods, more `63 `154 `40 generally represented `176 fewer bits than less `63 symbols. `16 `109 `52 `46 efficiently implemented, `85 a `56 `101 time linear `167 `161 number `118 input `181 `99 these `181 `40 sorted.[2] However, although `120 `35 `110 `78 `154 separately, `15 coding `104 not always `120 `35 all `65 `110 – it `104 replaced `186 arithmetic coding[3] `122 asymmetric numeral systems[4] `99 a better `65 ratio `104 required. History `18 1951, `11 `5 `15 `37 his MIT `102 theory classmates `182 given `161 choice `118 a `158 `126 `122 a `84 exam. `28 professor, Robert M. `12 assigned a `158 `126 on `161 problem `118 `85 `161 `113 efficient `48 `57 Huffman, unable `167 prove any codes `182 `161 `113 efficient, `180 about `167 give `173 `37 start studying `86 `161 `84 when `95 hit upon `161 idea `118 `176 a frequency-sorted `48 `169 `37 quickly proved `165 `109 `161 `113 efficient.[5] `18 doing so, `15 outdid `12 who had worked `186 Claude Shannon `167 develop a similar `57 Building `161 `169 `89 `161 bottom `173 guaranteed optimality, unlike `161 top-down approach `118 Shannon–Fano coding. 